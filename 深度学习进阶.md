# 深度学习进阶

## 自然语言和单词的分布式表示

### 同义词词典

- WordNet

	- 在自然语言处理领域，最著名的同义词词典是 WordNet。WordNet是普林斯顿大学于1985年开始开发的同义词词典，迄今已用于许多研究，并活跃于各种自然语言处理应用中。
	- 使用 WordNet，可以获得单词的近义词，或者利用单词网络。使用单词网络，可以计算单词之间的相似度。

- 同义词词典的问题

	- 难以顺应时代变化
	- 人力成本高
	- 无法表示单词的微妙差异

### 基于计数的方法

- 基于 Python 的语料库的预处理

  ```python
  def preprocess(text):
  	text = text.lower()
  	text = text.replace('.', ' .')
    words = text.split(' ')
  	word_to_id = {}
    id_to_word = {}
    
    for word in words:
  		if word not in word_to_id:
      	new_id = len(word_to_id)
        word_to_id[word] = new_id
        id_to_word[new_id] = word
        
  	corpus = np.array([word_to_id[w] for w in words])
    
  	return corpus, word_to_id, id_to_word
  ```

- 单词的分布式表示

	- 单词的分布式表示将单词表示为固定长度的向量。这种向量的特征在于它是用密集向量表示的。密集向量的意思是，向量的各个元素(大多数)是由非0实数表示的。 例如，三 维分布式表示是[0.21,-0.45,0.83]。

- 分布式假设

	- 在自然语言处理的历史中，用向量表示单词的研究有很多。如果仔细看一下这些研究，就会发现几乎所有的重要方法都基于一个简单的想法，这个想法就是“某个单词的含义由它周围的单词形成”

		- 我们将上下文的大小(即周围的单词有多少个)称为窗口大小(window size)。窗口大小为 1，上下文包含左右各 1 个单词；窗口大小为 2，上下文包含左右各 2 个单词，以此类推。

			- 我们将左右两边相同数量的单词作为上下文。但是，根据具体情况，也可以仅将左边的单词或者右边的单词作为上下文。 此外，也可以使用考虑了句子分隔符的上下文。

- 共现矩阵

  - 用矩阵汇总各个单词的上下文中包含的单词的频数

    - ![截屏2021-01-03 上午11.08.51](flink%E8%BF%9B%E9%98%B6.assets/%E6%88%AA%E5%B1%8F2021-01-03%20%E4%B8%8A%E5%8D%8811.08.51.png)
    - 这个表格的各行对应相应单词的向量。

  - ```python
    def create_co_matrix(corpus: list, vocab_size, window_size=1):
      """
      @param corpus: the word ids of the origin training text
      @param vocab_size: the total number of distinct words showed in the training text
      """
      corpus_size = len(corpus)
    	co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)
      
    	for idx, word_id in enumerate(corpus):
        for i in range(1, window_size + 1):
    			left_idx = idx - i
          right_idx = idx + i
    			if left_idx >= 0:
    				left_word_id = corpus[left_idx]
            co_matrix[word_id, left_word_id] += 1
    			if right_idx < corpus_size:
    				right_word_id = corpus[right_idx]
    				co_matrix[word_id, right_word_id] += 1
            
    	return co_matrix
    ```

- 向量间的相似度

  - 余弦相似度

    ```python
    def cos_similarity(x, y, eps=1e-8):
    	nx = x / (np.sqrt(np.sum(x ** 2)) + eps)
      ny = y / (np.sqrt(np.sum(y ** 2)) + eps)
      
      return np.dot(nx, ny)
    ```

  - 相似单词的排序

    ```python
    def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):
      
      # ❶取出查询词
    	if query not in word_to_id: 
        print('%s is not found' % query)
        return
      
    	print('\n[query] ' + query)
      query_id = word_to_id[query]
      query_vec = word_matrix[query_id]
                  
    	# ❷计算余弦相似度
    	vocab_size = len(id_to_word)
      similarity = np.zeros(vocab_size)
      for i in range(vocab_size):
    		similarity[i] = cos_similarity(word_matrix[i], query_vec)
        
      # ❸基于余弦相似度，按降序输出值
      # 将similarity 数组中的元素索引按降序重新排列，并输出顶部的单词。这里使用argsort()方法对数组的索引进行了重排。这个argsort() 方法可以按升序对 NumPy数组的元素进行排序(不过，返回值是数组的索引)
      count = 0
      for i in (-1 * similarity).argsort():
      	if id_to_word[i] == query: continue
      	print(' %s: %s' % (id_to_word[i], similarity[i]))
      	count += 1
      	if count >= top:
      		return
    ```
- 点互信息

  - $\operatorname{PMI}(x, y)=\log _{2} \frac{P(x, y)}{P(x) P(y)}=\log _{2} \frac{\frac{C(x, y)}{N}}{\frac{C(x)}{N} \frac{C(y)}{N}}=\log _{2} \frac{\boldsymbol{C}(x, y) \cdot N}{\boldsymbol{C}(x) \boldsymbol{C}(y)}$

    - $PPMI(x, y) = max (0, PMI(x, y))$

    ```python
    def ppmi(C, verbose=False, eps=1e-8):
    	M = np.zeros_like(C, dtype=np.float32)
      # the total sum
      N = np.sum(C)
      # the sum of columns
    	S = np.sum(C, axis=0)
      # the number of elemnets in C
    	total = C.shape[0] * C.shape[1]
      
    	cnt = 0
      verbose=False
      eps=1e-8
      
    	for i in range(C.shape[0]):
    		for j in range(C.shape[1]):
          # calculate each element in M
    			pmi = np.log2(C[i, j] * N / (S[j]*S[i]) + eps)
          M[i, j] = max(0, pmi)
          
    			if verbose:
            cnt += 1
    				if cnt % (total//100+1) == 0:
          		print('%.1f%% done' % (100*cnt/total))
      
    	return M
    ```

    

- 降维

  - $\boldsymbol{X}=\boldsymbol{U S V}^{\mathrm{T}}$

    - SVD 将任意的矩阵 X 分解为 U、S、V 这 3 个矩阵的乘积，其中 U和V是列向量彼此正交的正交矩阵，S是除了对角线元素以外其余元素均为 0 的对角矩阵。

    ```python
    U, S, V = np.linalg.svd(W)
    ```

  - 原先的稀疏向量 W 经过 SVD 被转化成了密集向量 U。
  如果要对这个密集向量降维，比如把它降维到二维向量，取出前两个元素即可。

## word2vec

### 基于推理的方法和神经网络

- 基于计数的方法的问题

  - 基于计数的方法根据一个单词周围的单词的出现频数来表示该单词。具体来说，先生成所有单词的共现矩阵，再对这个矩阵进行 SVD，以获得密集向量(单词的分布式表示)。但是，基于计数的方法在处理大规模语料库时会出现问题。
  - 在现实世界中，语料库处理的单词数量非常大。比如，据说英文的词汇量超过 100 万个。如果词汇量超过 100 万个，那么使用基于计数的方法就需要生成一个 100 万 × 100 万的庞大矩阵，但对如此庞大的矩阵执行 SVD 显然是不现实的。
  - 对于一个 n × n 的矩阵，SVD 的复杂度是 O(n^3)，这表示计算量与 n 的立方成比例增长。如此大的计算成本，即便是超级计算机也无法胜任。实际上，利用近似方法和稀疏矩阵的性质，可以在一定程度上提高处理速度，但还是需要大量的计算资源和时间。

  - 基于计数的方法使用整个语料库的统计数据(共现矩阵和 PPMI 等)， 通过一次处理(SVD 等)获得单词的分布式表示。而基于推理的方法使用神经网络，通常在 mini-batch 数据上进行学习。这意味着神经网络一次只需要看一部分学习数据(mini-batch)，并反复更新权重。

- 基于推理的方法的概要

	- 基于推理的方法和基于计数的方法一样，也基于分布式假设。分布式假设假设“单词含义由其周围的单词构成”。基于推理的方法将这一假设归结为了上面的预测问题。由此可见，不管是哪种方法，如何对基于分布式假设的“单词共现”建模都是最重要的研究主题。


- 神经网络中单词的处理方法

	- 从现在开始，我们将使用神经网络来处理单词。但是，神经网络无法直接处理 you 或 say 这样的单词，要用神经网络处理单词，需要先将单词转化为固定长度的向量。对此，一种方式是将单词转换为 one-hot 表示。在 one-hot 表示中，只有一个元素是 1，其他元素都是 0。

### 简单的 word2vec

- CBOW 模型的推理

  - ![截屏2021-01-04 下午3.54.09](flink%E8%BF%9B%E9%98%B6.assets/%E6%88%AA%E5%B1%8F2021-01-04%20%E4%B8%8B%E5%8D%883.54.09.png)

    - 它有两个输入层，经过中间层到达输出层。这里，从输入层到中间层的变换由相同的全连接层(权重为$Win$)完成， 从中间层到输出层神经元的变换由另一个全连接层(权重为$Wout$)完成。

    - 这里，因为我们对上下文仅考虑两个单词，所以输入层有两个。 如果对上下文考虑 N 个单词，则输入层会有 N 个。
    - 此时，中间层的神经元是各个输入层经全连接层变换后得到的值的“平均”。就上面的例子而言，经全连接 层变换后，第 1 个输入层转化为 h1，第 2 个输入层转化为 h2，那么中间层的神经元是 $1 / 2 * (h1 + h2)$。
    - 权重 Win 的各行保存着各个单词的分布式表示

    	- 中间层的神经元数量比输入层少这一点很重要。中间层需要将预测单词所需的信息压缩保存，从而产生密集的向量表示。这时，中间层被写入了我们人类无法解读的代码，这相当于“编码”工作。而从中间层的信息获得期望结果的过程则称为“解码”。这一过程将被编码的信息复原为我们可以理解的形式。

    ```python
    # 样本的上下文数据
    c0 = np.array([[1, 0, 0, 0, 0, 0, 0]])
    c1 = np.array([[0, 0, 1, 0, 0, 0, 0]])
    
    # 权重的初始值
    W_in = np.random.randn(7, 3)
    W_out = np.random.randn(3, 7)
    
    # 生成层
    in_layer0 = MatMul(W_in)
    in_layer1 = MatMul(W_in)
    out_layer = MatMul(W_out)
    
    # 正向传播
    h0 = in_layer0.forward(c0)
    h1 = in_layer1.forward(c1)
    h = 0.5 * (h0 + h1)
    s = out_layer.forward(h)
    ```

- CBOW 模型的学习

	- CBOW 模型的学习就是调整权重，以使预测准确。其结果是，权重$Win$(确切地说是$Win$和$Wout$两者)学习到蕴含单词出现模式的向量。

		- CBOW 模型只是学习语料库中单词的出现模式。如果语料库不一样，学习到的单词的分布式表示也不一样。比如，只使用“体育”相关的文章得到的单词的分布式表示，和只使用“音乐”相关的文章得 到的单词的分布式表示将有很大不同。

- word2vec 的权重和分布式表示

	- word2vec 中使用的网络有两个权重，分别是输入侧的全连接层的权重($Win$)和输出侧的全连接层的权重($Wout$)。一般而言，输入侧的权重$Win$的每一行对应于各个单词的分布式表示。另外，输出侧的权重$Wout$也同样保存了对单词含义进行了编码的向量。输出侧的权重在列方向上保存了各个单词的分布式表示。

		- 那么，我们最终应该使用哪个权重作为单词的分布式表示呢?这里有三个选项。

- 只使用输入侧的权重
			- 只使用输出侧的权重
			- 同时使用两个权重
		
	- 其中一个方式就是简单地将这两个权重相加。
	
- 学习数据的准备

  - 上下文和目标词

    ```python
    def create_contexts_target(corpus, window_size=1):
      target = corpus[window_size:-window_size]
      contexts = []
      cs = []
      
    	for idx in range(window_size, len(corpus)-window_size): 
    		for t in range(-window_size, window_size + 1):
          if t == 0:
    				continue
    			cs.append(corpus[idx + t])
        contexts.append(cs)
    
    	return np.array(contexts), np.array(target)
    ```

  - 转化为 one-hot 表示

- CBOW 模型的实现

  ```python
  class SimpleCBOW:
    
  	def __init__(self, vocab_size, hidden_size):
  		V, H = vocab_size, hidden_siz
      
  		# 初始化权重
      # 所有的输入节点公用一个权重矩阵
      W_in = 0.01 * np.random.randn(V, H).astype('f')
      W_out = 0.01 * np.random.randn(H, V).astype('f')
  
  		# 生成层
      self.in_layer0 = MatMul(W_in)
      self.in_layer1 = MatMul(W_in)
      self.out_layer = MatMul(W_out)
      self.loss_layer = SoftmaxWithLoss()
  
  		# 将所有的权重和梯度整理到列表中
  		layers = [self.in_layer0, self.in_layer1, self.out_layer]
      self.params, self.grads = [], []
  		for layer in layers:
  			self.params += layer.params
        self.grads += layer.grads
  
  	def forward(self, contexts, target):
      """
      @param: 目标词的上下文的ont-hot向量
      @param: 目标词的one-hot向量
      """
  		h0 = self.in_layer0.forward(contexts[:, 0])
      h1 = self.in_layer1.forward(contexts[:, 1])
  		h = (h0 + h1) * 0.5
  		score = self.out_layer.forward(h)
  		loss = self.loss_layer.forward(score, target)
      
      return loss
    
  	def backward(self, dout=1):
  		ds = self.loss_layer.backward(dout)
      da = self.out_layer.backward(ds)
  		da *= 0.5
      self.in_layer1.backward(da)
      self.in_layer0.backward(da)
  
      return None
  ```

- CBOW 模型和概率

	- $P\left(w_{t} \mid w_{t-1}, w_{t+1}\right)$
	- $L=-\frac{1}{T} \sum_{t=1}^{T} \log P\left(w_{t} \mid w_{t-1}, w_{t+1}\right)$

		- CBOW 模型学习的任务就是让该式表示的损失函数尽可能地小。 那时的权重参数就是我们想要的单词的分布式表示。

- skip-gram 模型

  - ![截屏2021-01-04 下午6.08.27](flink%E8%BF%9B%E9%98%B6.assets/%E6%88%AA%E5%B1%8F2021-01-04%20%E4%B8%8B%E5%8D%886.08.27.png)
  - skip-gram 模型的输入层只有一个，输出层的数量则 与上下文的单词个数相等。因此，首先要分别求出各个输出层的损失(通过 Softmax with Loss 层等)，然后将它们加起来作为最后的损失。
  - $P\left(w_{t-1}, w_{t+1} \mid w_{t}\right)$

  	- 在 skip-gram 模型中，假定上下文的单词之间没有相关性(正确地说是假定“条件独立”)，将式如下进行分解

  - $P\left(w_{t-1}, w_{t+1} \mid w_{t}\right)=P\left(w_{t-1} \mid w_{t}\right) P\left(w_{t+1} \mid w_{t}\right)$
  - $\begin{aligned}
  L &=-\log P\left(w_{t-1}, w_{t+1} \mid w_{t}\right) \\
  &=-\log P\left(w_{t-1} \mid w_{t}\right) P\left(w_{t+1} \mid w_{t}\right) \\
  &=-\left(\log P\left(w_{t-1} \mid w_{t}\right)+\log P\left(w_{t+1} \mid w_{t}\right)\right)
  \end{aligned}$
  - skip-gram 模型的损失函数先分别求出各个上下文对应的损失，然后将它们加在一起。
  	
  	- 如果扩展到整个语料库
  	
  		- $L=-\frac{1}{T} \sum_{t=1}^{T}\left(\log P\left(w_{t-1} \mid w_{t}\right)+\log P\left(w_{t+1} \mid w_{t}\right)\right)$
  - 那么，我们应该使用 CBOW 模型和 skip-gram 模型中的哪一个呢？

  	- 答案应该是 skip-gram 模型。这是因为，从单词的分布式表示的准确度来看， 在大多数情况下，skip-gram 模型的结果更好。特别是随着语料库规模的增大，在低频词和类推问题的性能方面，skip-gram 模型往往会有更好的表现。
  - 就学习速度而言， CBOW 模型比 skip-gram 模型要快。这是因为 skip-gram 模型需要根据上下文数量计算相应个数的损失，计算成本变大。

- 基于计数与基于推理

	- 首先，我们考虑需要向词汇表添加新词并更新单词的分布式表示的场景。此时，基于计数的方法需要从头开始计算。即便是想稍微修改一下单词的分布式表示，也需要重新完成生成共现矩阵、进行 SVD 等一系列操作。 相反，基于推理的方法(word2vec)允许参数的增量学习。具体来说，可以将之前学习到的权重作为下一次学习的初始值，在不损失之前学习到的经验的情况下，高效地更新单词的分布式表示。在这方面，基于推理的方法(word2vec)具有优势。
	- 其次，两种方法得到的单词的分布式表示的性质和准确度有什么差异呢?就分布式表示的性质而言，基于计数的方法主要是编码单词的相似性，而 word2vec(特别是 skip-gram模型)除了单词的相似性以外，还能理解更复杂的单词之间的模式。关于这一点，word2vec 因能解开“king − man + woman = queen”这样的类推问题而知名。
- 这里有一个常见的误解，那就是基于推理的方法在准确度方面优于基于计数的方法。实际上，有研究表明，就单词相似性的定量评价而言，基于推理的方法和基于计数的方法难分上下。
	
	- 2014年发表的题为“Don’t count, predict!”(不要计数，要预测!) 的论文系统地比较了基于计数的方法和基于推理的方法，并给出了基于推理的方法在准确度上始终更好的结论。但是，之后又有 其他的论文提出，就单词的相似性而言，结论高度依赖于超参数， 基于计数的方法和基于推理的方法难分胜负。

## word2vec 的高速化

### Embedding 层 

- ![截屏2021-01-04 下午9.50.33](flink%E8%BF%9B%E9%98%B6.assets/%E6%88%AA%E5%B1%8F2021-01-04%20%E4%B8%8B%E5%8D%889.50.33.png)
- 如果语料库的词汇量有 100 万个，则单词的 one-hot 表示的维数也会是 100 万，我们需要计算这个巨大向量和权重矩阵的乘积。但是，上图所做的无非是将矩阵的某个特定的行取出来。因此，直觉上将单词转化为 one-hot 向量的处理和 MatMul 层中的矩阵乘法似乎没有必要。
- 现在，我们创建一个从权重参数中抽取“单词 ID 对应行”的 层，这里我们称之为 Embedding 层。顺便说一句，Embedding 来自“词嵌入”(word embedding)这一术语。也就是说，在这个 Embedding 层存放词嵌入(分布式表示)。

### Embedding 层的实现

- Embedding 层的正向传播和反向传播处理的概要(Embedding 层记为 Embed)

  - ![截屏2021-01-04 下午9.52.27](flink%E8%BF%9B%E9%98%B6.assets/%E6%88%AA%E5%B1%8F2021-01-04%20%E4%B8%8B%E5%8D%889.52.27.png)
  - Embedding 层的正向传播只是从权重矩阵 W 中提取特定的行，并将该特定行的神经元原样传给下一层。因此，在反向传播时，从上一层(输出侧的层)传过来的梯度将原样传给下一层(输入侧的层)。不过，从上一层传来的梯度会被应用到权重梯度 dW 的特定行。

  ```python
  def backward(self, dout):
    dW, = self.grads
    dW[...] = 0
    for i, word_id in enumerate(self.idx):
      dW[word_id] += dout[i]
  # 或者
  # np.add.at(dW, self.idx, dout)
  # 通常情况下，NumPy 的内置方法比 Python 的 for循环处理更快。 这是因为 NumPy 的内置方法在底层做了高速化和提高处理效率的优化。因此，上面的代码如果使用 np.add.at()来实现，效率会比使用 for 循环处理高得多。
  	
    return None
  ```


### 中间层之后的计算问题

- 中间层的神经元和权重矩阵(Wout)的乘积
- Softmax 层的计算

### 从多分类到二分类

- 负采样

	- 这个方法的关键思想在于二分类 (binary classification)，更准确地说，是用二分类拟合多分类(multiclass
classification)，这是理解负采样的重点。
- 输出层的神经元仅有一个。因此，要计算中间层和输出侧的权重矩阵的乘积，只需
		
	- 输出侧的权重 Wout 中保存了各个单词 ID 对应的单词向量。此处，我们提取 say 这个单词向量，再求这个向量和中间层神经元的内积，这就是最终的得分。

### sigmoid 函数和交叉熵误差

- ![截屏2021-01-04 下午10.12.48](flink%E8%BF%9B%E9%98%B6.assets/%E6%88%AA%E5%B1%8F2021-01-04%20%E4%B8%8B%E5%8D%8810.12.48.png)
- 值得注意的是反向传播的 y − t 这个值。y 是神经网络输出的概率，t 是正确解标签，y − t 正好是这两个值的差。这意味着，当正确解 签是 1 时，如果 y 尽可能地接近 1(100%)，误差将很小。反过来，如果 y 远离 1，误差将增大。随后，这个误差向前面的层传播，当误差大时，模型学习得多；当误差小时，模型学习得少 。

### 多分类到二分类的实现

- 进行二分类的 CBOW 模型的全貌图

  - ![截屏2021-01-04 下午10.20.22](flink%E8%BF%9B%E9%98%B6.assets/%E6%88%AA%E5%B1%8F2021-01-04%20%E4%B8%8B%E5%8D%8810.20.22.png)
  - 这里，将中间层的神经元记为 h，并计算它与输出侧权重 $Wout$ 中的单词 say 对应的单词向量的内积。然后，将其输出输入 Sigmoid with Loss 层， 得到最终的损失
  	- 在图中，向Sigmoid with Loss层输入正确解标签1，这意味着现在正在处理的问题的答案是“Yes”。当答案是“No”时， 向Sigmoid with Loss层输入0。

- 为了便于理解后面的内容，我们把图后半部分进一步简化。为此，我们引入 Embedding Dot 层，该层将图中的 Embedding 层和 dot运算(内积)合并起来处理。

  ```python
  class EmbeddingDot:
  	def __init__(self, W):
  		self.embed = Embedding(W)
      self.params = self.embed.params
      self.grads = self.embed.grads
      self.cache = None
      
  	def forward(self, h, idx):
  		target_W = self.embed.forward(idx)
      out = np.sum(target_W * h, axis=1)
  		self.cache = (h, target_W)
      
      return out
    
  	def backward(self, dout):
  		h, target_W = self.cache
  		dout = dout.reshape(dout.shape[0], 1)
      
  		dtarget_W = dout * h
      self.embed.backward(dtarget_W)
      dh = dout * target_W
      
  		return dh
  ```

### 负采样

- 为了把多分类问题处理为二分类问题，对于“正确答案”(正例)和“错误答案”(负例)，都需要能够正确地进行分类(二分类)。因此，需要同时考虑正例和负例。

	- 那么，我们需要以所有的负例为对象进行学习吗?答案显然是“No”。 如果以所有的负例为对象，词汇量将暴增至无法处理(更何况本章的目的本来就是解决词汇量增加的问题)。为此，作为一种近似方法，我们将选择若 干个(5 个或者 10 个)负例(如何选择将在下文介绍)。也就是说，只使用少数负例。这就是负采样方法的含义。

		- 总而言之，负采样方法既可以求将正例作为目标词时的损失，同时也可以采样(选出)若干个负例，对这些负例求损失。然后，将这些数据(正例和采样出来的负例)的损失加起来，将其结果作为最终的损失。

### 负采样的采样方法

- 基于语料库中单词使用频率的采样方法会先计算语料库中各个单词的出现频率，并将其表示为“概率分布”，然后使用这个概率分布对单词进行采样。

	- 基于语料库中各个单词的出现次数求出概率分布后，只需根据这个概率分布进行采样就可以了。通过根据概率分布进行采样，语料库中经常出现的单词将容易被抽到，而“稀有单词”将难以被抽到。

- 负采样应当尽可能多地覆盖负例单词，但是考虑到计算的复杂度， 有必要将负例限定在较小范围内(5 个或者 10 个)。这里，如果只选择稀有单词作为负例会怎样呢? 
    - 结果会很糟糕。因为在现实问题中， 稀有单词基本上不会出现。也就是说，处理稀有单词的重要性较低。 相反，处理好高频单词才能获得更好的结果。


- word2vec 中提出的负采样对刚才的概率分布增加了一个步骤。对原来的概率分布取 0.75 次方。

	- $P^{\prime}\left(w_{i}\right)=\frac{P\left(w_{i}\right)^{0.75}}{\sum_{j}^{n} P\left(w_{j}\right)^{0.75}}$

		- 那么，为什么我们要进行上式的变换呢?这是为了防止低频单词被忽略。更准确地说，通过取 0.75 次方，低频单词的概率将稍微变高。

### 负采样的实现

```python
class NegativeSamplingLoss:
    def __init__(self, W: np.ndarray, corpus: list, power=0.75, sample_size=5):
    """
    @param W: embedding 权重矩阵
    @param corpus: text中的单词ids
    """
    self.sample_size = sample_size
    self.sampler = UnigramSampler(corpus, power, sample_size)
    self.loss_layers = [SigmoidWithLoss() for _ in range(sample_size+ 1)]
    self.embed_dot_layers = [EmbeddingDot(W) for _ in range(sample_size + 1)]
    self.params, self.grads = [], []
    for layer in self.embed_dot_layers:
    	self.params += layer.params
      self.grads += layer.grads
    
    def forward(self, h, target):
			 batch_size = target.shape[0]
			 negative_sample = self.sampler.get_negative_sample(target)
      
			 # 正例的正向传播
			 score = self.embed_dot_layers[0].forward(h, target)
			 correct_label = np.ones(batch_size, dtype=np.int32)
 			 loss = self.loss_layers[0].forward(score, correct_label)
      
			 # 负例的正向传播
			 negative_label = np.zeros(batch_size, dtype=np.int32)
			 for i in range(self.sample_size):
          negative_target = negative_sample[:, i]
          score = self.embed_dot_layers[1 + i].forward(h, negative_target)
          loss += self.loss_layers[1 + i].forward(score, negative_label)
          
          return loss
        
		def backward(self, dout=1):
      dh = 0
      for l0, l1 in zip(self.loss_layers, self.embed_dot_layers):
        dscore = l0.backward(dout)
        dh += l1.backward(dscore)
        
        	return dh
```

### CBOW 模型的实现

```python
class CBOW:
    def __init__(self, vocab_size, hidden_size, window_size, corpus):
    V, H = vocab_size, hidden_size
    
		# 初始化权重
		W_in = 0.01 * np.random.randn(V, H).astype('f')
    W_out = 0.01 * np.random.randn(V, H).astype('f')

		# 生成层
    self.in_layers = []
		for i in range(2 * window_size):
			layer = Embedding(W_in) # 使用Embedding层
			self.in_layers.append(layer)
		self.ns_loss = NegativeSamplingLoss(W_out, corpus, power=0.75, sample_size=5)

		# 将所有的权重和梯度整理到列表中
		layers = self.in_layers + [self.ns_loss]
    self.params, self.grads = [], []
		for layer in layers:
			self.params += layer.params
      self.grads += layer.grads

		# 将单词的分布式表示设置为成员变量 self.word_vecs = W_in
		def forward(self, contexts, target):
      """
      虽然 forward (contexts, target) 方法取的参数仍是上下文和目标词，但是它们是单词 ID 形式的(上一章中使			用的是 one-hot 向量，不是单词ID)
      """
      h= 0
    	for i, layer in enumerate(self.in_layers):
        h += layer.forward(contexts[:, i])
    		h *= 1 / len(self.in_layers)
    	loss = self.ns_loss.forward(h, target)
      	return loss
      
    def backward(self, dout=1):
      dout = self.ns_loss.backward(dout)
      dout *= 1 / len(self.in_layers)
      for layer in self.in_layers:
        layer.backward(dout)

      return Non
```

### word2vec 的应用例

- 在解决自然语言处理任务时，一般不会使用 word2vec 从零开始学习单词的分布式表示，而是先在大规模语料库(Wikipedia、Google News 等文本数据)上学习，然后将学习好的分布式表示应用于某个单独的任务。比如， 在文本分类、文本聚类、词性标注和情感分析等自然语言处理任务中，第一步的单词向量化工作就可以使用学习好的单词的分布式表示。在几乎所有类型的自然语言处理任务中，单词的分布式表示都有很好的效果。

	- 单词的分布式表示的优点是可以将单词转化为固定长度的向量。另外，使用单词的分布式表示，也可以将文档(单词序列)转化为固定长度的向量。目前，关于如何将文档转化为固定长度的向量，相关研究已经进行了很多，最简单的方法是，把文档的各个单词转化为分布式表示，然后求它们的总和。这是一种被称为 bag-of-words 的不考虑单词顺序的模型(思想)。此外，使用即将在第 5 章中说明的循环神经网络，可以以更加优美的方式利用 word2vec 的单词的分布式表示来将文档转化为固定长度的向量。

		- 单词的分布式表示的学习和机器学习系统的学习通常使用不同的数据集独立进行。比如，单词的分布式表示使用 Wikipedia 等通用语料库预先学习好，然后机器学习系统(SVM 等) 再使用针对当前问题收集到的数据进行学习。但是，如果当前我们面对的问题存在大量的学习数据，则也可以考虑从零开始同时进行单词的分布式表示和机器学习系统的学习。
		- 正如上述情感分析的例子那样，在现实世界中，单词的分布式表示往往被用在具体的应用中。我们最终想要的是一个高精度的系统。这里我们必须考虑到的是，这个系统(比如情感分析系统)是由多个子系统组成的。所谓多个子系统，拿刚才的例子来说，包括生成单词的分布式表示的系统(word2vec)、对特定问题进行分类的系统(比如进行情感分类的 SVM 等)。
		
		    - 单词的分布式表示的学习和分类系统的学习有时可能会分开进行。在这种情况下，如果要调查单词的分布式表示的维数如何影响最终的精度，首先需要进行单词的分布式表示的学习，然后再利用这个分布式表示进行另一个机器学习系统的学习。换句话说，在进行两个阶段的学习之后，才能进行评价。在这种情况下，由于需要调试出对两个系统都最优的超参数，所以非常费时。


### 单词向量的评价方法 

- 单词相似度的评价通常使用人工创建的单词相似度评价集来评估。比如，cat 和 animal 的相似度是 8，cat 和 car 的相似度是 2......类似这样， 用 0 ~ 10 的分数人工地对单词之间的相似度打分。然后，比较人给出的分数和 word2vec 给出的余弦相似度，考察它们之间的相关性。
- 类推问题的评价是指，基于诸如“king : queen = man : ?”这样的类推问题，根据正确率测量单词的分布式表示的优劣。

	- 模型不同，精度不同(根据语料库选择最佳的模型)
	- 语料库越大，结果越好(始终需要大数据)
	- 单词向量的维数必须适中(太大会导致精度变差)

## RNN

### Backpropagation Through Time

- 将循环展开后的 RNN 可以使用(常规的)误差反向传播法。换句话说，可以通过先进行正向传播，再进行反向传播的方式求目标梯度。因为这里的误差反向传播法是“按时间顺序展开的神经网络的误差反向传播法”，所以称为 Backpropagation Through Time(基于时间的反向传播)，简称 BPTT。

	- 通过该 BPTT，RNN 的学习似乎可以进行了，但是在这之前还有一个必须解决的问题，那就是学习长时序数据的问题。因为随着时序数据的时间跨度的增大，BPTT 消耗的计算机资源也会成比例地增大。另外，反向传播的梯度也会变得不稳定。

### Truncated BPTT

- 在处理长时序数据时，通常的做法是将网络连接截成适当的长度。具体来说，就是将时间轴方向上过长的网络在合适的位置进行截断，从而创建多个小型网络，然后对截出来的小型网络执行误差反向传播法，这个方法称 为 Truncated BPTT(截断的 BPTT)。

	- 在 Truncated BPTT 中，网络连接被截断，但严格地讲，只是网络的反向传播的连接被截断，正向传播的连接依然被维持，这一点很重要。也就是说，正向传播的信息没有中断地传播。与此相对，反向传播则被截断为适当的长度，以被截出的网络为单位进行学习。

- 我们之前看到的神经网络在进行 mini-batch 学习时，数据都是随机选择的。但是，在RNN执行Truncated BPTT时，数据需要按顺序输入。（不同样本之间保持原来的序列顺序）

### Truncated BPTT 的 mini-batch 学习

- ![截屏2021-01-08 下午2.57.33](flink%E8%BF%9B%E9%98%B6.assets/%E6%88%AA%E5%B1%8F2021-01-08%20%E4%B8%8B%E5%8D%882.57.33.png)

    - 到目前为止，我们在探讨 Truncated BPTT 时，并没有考虑 mini- batch 学习。换句话说，我们之前的探讨对应于批大小为 1 的情况。为了执行 mini-batch 学习，需要考虑批数据，让它也能按顺序输入数据。因此，在输入数据的开始位置，需要在各个批次中进行“偏移”。

### RNN 层的实现

```python
class RNN:
	def __init__(self, Wx, Wh, b):
		self.params = [Wx, Wh, b]
		self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]
		self.cache = None
    
	def forward(self, x, h_prev):
		Wx, Wh, b = self.params
    
		t = np.dot(h_prev, Wh) + np.dot(x, Wx) + b
    h_next = np.tanh(t)
    
		self.cache = (x, h_prev, h_next)
    
    return h_next
  
	def backward(self, dh_next):
		Wx, Wh, b = self.params
		x, h_prev, h_next = self.cache
    
		dt = dh_next * (1 - h_next ** 2)
    db = np.sum(dt, axis=0)
		dWh = np.dot(h_prev.T, dt)
    dh_prev = np.dot(dt, Wh.T)
		dWx = np.dot(x.T, dt)
    dx = np.dot(dt, Wx.T)
    
		self.grads[0][...] = dWx
    self.grads[1][...] = dWh
    self.grads[2][...] = db
    
		return dx, dh_prev
```

### Time RNN 层的实现

```python
class TimeRNN:
  """
  考虑到TimeRNN类的扩展性，将设定Time RNN层的隐藏状态的方法实现为set_state(h)。另外，将重设隐藏状态的方法实	现为reset_state()。
  """
  def __init__(self, Wx, Wh, b, stateful=False):
		# 所有time step的RNN神经元公用一份权重矩阵
  	self.params = [Wx, Wh, b]
  	self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]
  	self.layers = None
  	self.h, self.dh = None, None
    self.stateful = stateful
    
  def set_state(self, h):
    self.h = h
    
  def reset_state(self):
    self.h = None
    
  def forward(self, xs):
    Wx, Wh, b = self.params
    N, T, D = xs.shape
    D, H = Wx.shape
    self.layers = []
    hs = np.empty((N, T, H), dtype='f')
    
    if not self.stateful or self.h is None:
      self.h = np.zeros((N, H), dtype='f')
      
    for t in range(T):
    	layer = RNN(*self.params)
    	self.h = layer.forward(xs[:, t, :], self.h)
      hs[:, t, :] = self.h
      self.layers.append(layer)
      
    return hs
  
  def backward(self, dhs):
    Wx, Wh, b = self.params
    N, T, H = dhs.shape
  	D, H = Wx.shape
  	dxs = np.empty((N, T, D), dtype='f')
    dh = 0
    # grads的元素为：[grads_of_Wx, grads_of_Wh, grads_of_b]
  	grads = [0, 0, 0]
    
  	for t in reversed(range(T)):
  		layer = self.layers[t]
  		dx, dh = layer.backward(dhs[:, t, :] + dh)
      dxs[:, t, :] = dx
      
			for i, grad in enumerate(layer.grads):
      	grads[i] += grad
        
      for i, grad in enumerate(grads):
        self.grads[i][...] = grad
        # 左边第一个RNN神经元向前传播的值
        self.dh = dh
      
    return dxs
```

- 正向传播的 forward(xs) 方法从下方获取输入 xs，xs 囊括了 T 个时序数据。因此，如果批大小是 N，输入向量的维数是 D，则 xs 的形状为 (N,T,D)。 在首次调用时(self.h 为 None 时)，RNN 层的隐藏状态 h 由所有元素均为 0 的矩阵初始化。另外，在成员变量 stateful 为 False 的情况下，h 将总是被重置为零矩阵。
- 在主体实现中，首先通过 hs=np.empty((N, T, H), dtype='f') 为输出准备一个“容器”。接着，在 T 次 for 循环中，生成 RNN 层，并将其添加到成员变量 layers 中。然后，计算 RNN 层各个时刻的隐藏状态，并存放在 hs的对应索引(时刻)中。
- 如果调用Time RNN层的forward()方法，则成员变量h中将存放最后一个RNN层的隐藏状态。在stateful为True的情况下，在下一次调用 forward() 方法时，刚才的成员变量 h 将被继续使用。而在 stateful 为 False 的情况下，成员变量 h 将被重置为零向量。

- 在Time RNN层中有多个RNN层。另外，这些RNN层使用相同的权重。因此，Time RNN层的(最终)权重梯度是各个RNN 层的权重梯度之和。

### 处理时序数据的层的实现

- RNNLM 的全貌图

  - ![截屏2021-01-12 上午10.35.02](flink%E8%BF%9B%E9%98%B6.assets/%E6%88%AA%E5%B1%8F2021-01-12%20%E4%B8%8A%E5%8D%8810.35.02.png)

- RNNLM 层的实现

  ```python
  class SimpleRnnlm:
    def __init__(self, vocab_size, wordvec_size, hidden_size):
    	V, D, H = vocab_size, wordvec_size, hidden_size
      
      rn = np.random.randn
  		# 初始化权重
      embed_W = (rn(V, D) / 100).astype('f')
      rnn_Wx = (rn(D, H) / np.sqrt(D)).astype('f')
      rnn_Wh = (rn(H, H) / np.sqrt(H)).astype('f')
      rnn_b = np.zeros(H).astype('f')
      affine_W = (rn(H, V) / np.sqrt(H)).astype('f')
      affine_b = np.zeros(V).astype('f')
  
  		# 生成层
      self.layers = [
        TimeEmbedding(embed_W),
        TimeRNN(rnn_Wx, rnn_Wh, rnn_b, stateful=True),
        TimeAffine(affine_W, affine_b)
      ]
  		self.loss_layer = TimeSoftmaxWithLoss()
      self.rnn_layer = self.layers[1]
  
  		# 将所有的权重和梯度整理到列表中
      self.params, self.grads = [], []
      for layer in self.layers:
  			self.params += layer.params
      	self.grads += layer.grads
  	
  	def forward(self, xs, ts):
      for layer in self.layers:
  			xs = layer.forward(xs)
  		loss = self.loss_layer.forward(xs, ts)
        
      return loss
      
  	def backward(self, dout=1):
  		dout = self.loss_layer.backward(dout)
      for layer in reversed(self.layers):
  			dout = layer.backward(dout)
      
      return dout
    
  	def reset_state(self):
      self.rnn_layer.reset_state()
  ```

  - RNN 层和 Affine 层使用了“Xavier 初始值”。在上一层的节点数是 n 的情况下，使用标准差为√n 的分布作为Xavier初始值。

  - 在深度学习中，权重的初始值非常重要。同样，对 RNN 而言，权重的初始值也很重要。通过设置好的初始值，学习的进展和最终的精度都会有很大变化。本书此后都将使用 Xavier 初始值作为权重的初始值。另外，在语言模型的相关研究中，经常使用0.01 * np.random.uniform(...) 这样的经过缩放的均匀分布。

- 困惑度

	- 困惑度表示“概率的倒数”(这个解释在数据量为 1 时严格一 致)

		- 基于困惑度可以评价模型的预测性能。好的模型可以高概率地预测出正确单词，所以困惑度较小(困惑度的最小值是 1.0)；而差的模型只能低概率地预测出正确单词，困惑度较大。

		- 以上都是输入数据为 1 个时的困惑度。那么，在输入数据为多个的情况 下，结果会怎样呢?我们可以根据下面的式子进行计算。

			- $\begin{array}{c}
L=-\frac{1}{N} \sum_{n} \sum_{k} t_{n k} \log y_{n k} \\
\text { 困惑度 }=\mathrm{e}^{L}
\end{array}$

- RNNLM 的学习代码

  ```python
  # 设定超参数
  batch_size = 10
  wordvec_size = 100
  # RNN的隐藏状态向量的元素个数
  hidden_size = 100
  # Truncated BPTT的时间跨度大小
  time_size = 5
  lr = 0.1
  max_epoch = 100
  
  # 读入训练数据(缩小了数据集)
  corpus, word_to_id, id_to_word = ptb.load_data('train')
  corpus_size = 1000
  corpus = corpus[:corpus_size]
  vocab_size = int(max(corpus) + 1)
  xs = corpus[:-1] # 输入
  ts = corpus[1:] # 输出(监督标签)
  data_size = len(xs)
  print('corpus size: %d, vocabulary size: %d' % (corpus_size, vocab_size))
  
  # 学习用的参数
  # 一个batch, 会用到batch_size * time_size个词
  max_iters = data_size // (batch_size * time_size)
  time_idx = 0
  total_loss = 0
  loss_count = 0
  ppl_list = []
  
  # 生成模型
  model = SimpleRnnlm(vocab_size, wordvec_size, hidden_size)
  optimizer = SGD(lr)
  
  # ❶ 计算读入mini-batch的各笔样本数据的开始位置
  jump = (corpus_size - 1) // batch_size
  offsets = [i * jump for i in range(batch_size)]
  
  for epoch in range(max_epoch):
    for iter in range(max_iters):
  	# ❷ 获取mini-batch
  	batch_x = np.empty((batch_size, time_size), dtype='i')
    batch_t = np.empty((batch_size, time_size), dtype='i')
    for t in range(time_size):
  		for i, offset in enumerate(offsets):
  			batch_x[i, t] = xs[(offset + time_idx) % data_size]
        batch_t[i, t] = ts[(offset + time_idx) % data_size]
  			time_idx += 1
  
    # 计算梯度，更新参数
    loss = model.forward(batch_x, batch_t)
    model.backward()
    optimizer.update(model.params, model.grads)
    total_loss += loss
    loss_count += 1
  
    # ❸ 各个epoch的困惑度评价
    ppl = np.exp(total_loss / loss_count)
    print('| epoch %d | perplexity %.2f' % (epoch+1, ppl))
    ppl_list.append(float(ppl))
    total_loss, loss_count = 0, 0
  ```
## Gated RNN

### RNN 的问题

- 梯度消失和梯度爆炸 

	- RNN 层通过向过去传递“有意义的梯度”，能够学习时间方向上的依赖关系。此时梯度 （理论上）包含了那些应该学到的有意义的信息，通过将这些信息向过去传递，RNN 层学习长期的依赖关系。但是，如果这个梯度在中途变弱(甚至没有包含任何信息)，则权重参数将不会被更新。也就是说，RNN 层无法学习长期的依赖关系。不幸的是，随着时间的回溯，这个简单 RNN 未能避免梯度变小（梯度消失）或者梯度变大（梯度爆炸）的命运。

- 梯度消失和梯度爆炸的原因 

  - 这里考虑长度为 T 的时序数据，关注从第 T 个正确解标签传递出的梯度如何变化。就上面的问题来说，这相当于第 T 个正确解标签是 Tom 的情形。此时，关注时间方向上的梯度，可知反向传播的梯度流经 tanh、“+”和 MatMul（矩阵乘积）运算。

    - “+”的反向传播将上游传来的梯度原样传给下游，因此梯度的值不变。
    - y = tanh(x) 的导数的值小于 1.0，并且随着 x 远离 0，它的值在变小。这意味着，当反向传播的梯度经过 tanh 节点时，它的值会越来越小。因此，如果经过 tanh 函数 T 次，则梯度也会减小 T 次。

      - RNN 层的激活函数一般使用 tanh 函数，但是如果改为 ReLU 函数， 则有希望抑制梯度消失的问题(当 ReLU 的输入为 x 时，它的输出是 max(0, x))。这是因为，在 ReLU 的情况下，当 x 大于 0 时，反向传播将上游的梯度原样传递到下游，梯度不会“退化”。实际上，题为“Improving performance of recurrent neural network with relu nonlinearity”的论文就使用ReLU实现了性能改善。
      - 假定从上游传来梯度 dh，此时 MatMul 节点的反向传播通过矩阵乘积 dhWhT 计算梯度。之后，根据时序数据的时间步长，将这个计算重复相应次数。这里需要注意的是，每一次矩阵乘积计算都使用相同的 权重 Wh。
      - 在这里进行的实验中，梯度的大小或者呈指数级增加，或者呈指数级减小。
        - 为什么会出现这样的指数级变化呢?
          - 因为矩阵 Wh 被反复乘了 T 次。如果 Wh 是标量，则问题将很简单:
            - 当 Wh 大于 1 时，梯度呈指数级增加;
            - 当 Wh小于 1 时，梯度呈指数级减小。
            - 那么，如果 Wh 不是标量，而是矩阵呢?
                - 此时，矩阵的奇异值将成为指标。简单而言，矩阵的奇异值表示数据的离散程度。根据这个奇异值(更准确地说是多个奇异值中的最大值)是否大于 1，可以预测梯度大小的变化。
            - 如果奇异值的最大值大于 1，则可以预测梯度很有可能会呈指数级增加；而如果奇异值的最大值小于 1，则可以判断梯度会呈指数级减小。但是，并不是说奇异值比 1 大就一定会出现梯度爆炸。 也就是说，这是必要条件，并非充分条件。

- 梯度爆炸的对策 

  - 梯度裁剪

    - $\text { if } \begin{aligned}
      \|\hat{\boldsymbol{g}}\| & \geqslant \text { threshold: } \\
      \hat{\boldsymbol{g}} &=\frac{t h r e s h o l d}{\|\hat{\boldsymbol{g}}\|} \hat{\boldsymbol{g}}
      \end{aligned}$

    - ```python
      def clip_grads(grads, max_norm):
        total_norm = 0
      	for grad in grads:
      	total_norm += np.sum(grad ** 2)
      	total_norm = np.sqrt(total_norm)
        
      	rate = max_norm / (total_norm + 1e-6) 
        if rate < 1:
      		for grad in grads:
            grad *= rate
      ```

  - 梯度消失和 LSTM

    - ![截屏2021-01-13 上午11.26.08](flink%E8%BF%9B%E9%98%B6.assets/%E6%88%AA%E5%B1%8F2021-01-13%20%E4%B8%8A%E5%8D%8811.26.08.png)
    - LSTM 与 RNN 的接口的不同之处在于，LSTM 还有路径 c。这个 c 称为记忆单元(或者简称为“单元”)，相当于 LSTM 专用的记忆部门。
    	
    	- 记忆单元的特点是，仅在 LSTM 层内部接收和传递数据。也就是说， 记忆单元在 LSTM 层内部结束工作，不向其他层输出。而 LSTM 的隐藏状态 h 和 RNN 层相同，会被向上输出到其他层。

### LSTM 层的结构 

- ![截屏2021-01-13 上午11.42.28](flink%E8%BF%9B%E9%98%B6.assets/%E6%88%AA%E5%B1%8F2021-01-13%20%E4%B8%8A%E5%8D%8811.42.28-1285785.png)

	- 当前的记忆单元 ct 是基于 3 个输入 ct−1、ht−1 和 xt， 经过“某种计算”(后述)算出来的。这里的重点是隐藏状态 ht 要使用更新后的 ct 来计算。另外，这个计算是 ht = tanh(ct)，表示对 ct 的各个元素应用 tanh 函数。

		- 到目前为止，记忆单元 ct 和隐藏状态 ht 的关系只是按元素应用 tanh 函数。这意味着，记忆单元 ct 和隐藏状态 ht 的元素个数相同。 如果记忆单元 ct 的元素个数是 100，则隐藏状态 ht 的元素个数也是 100。

- 输出门的开合程度(流出比例)根据输入 xt 和上一个状态 ht−1 求出。此时进行的计算如下所示。这里在使用的权重参数和偏置的上标上添加了 output 的首字母 o。之后，我们也将使用上标表示门。另外， sigmoid 函数用 σ() 表示。

	- $\boldsymbol{o}=\sigma\left(\boldsymbol{x}_{t} \boldsymbol{W}_{x}^{(\mathrm{o})}+\boldsymbol{h}_{t-1} \boldsymbol{W}_{h}^{(\mathrm{o})}+\boldsymbol{b}^{(\mathrm{o})}\right)$

		- 将这个 o 和 tanh(ct) 的 对应元素的乘积作为 ht 输出
		- $\boldsymbol{h}_{t}=\boldsymbol{o} \odot \tanh \left(\boldsymbol{c}_{t}\right)$

- tanh 的输出是 −1.0 ~ 1.0 的实数。我们可以认为这个 −1.0 ~ 1.0 的 数值表示某种被编码的“信息”的强弱程度。而sigmoid函数的输出是 0.0~1.0 的实数，表示数据流出的比例。因此，在大多数情况下，门使用 sigmoid 函数作为激活函数，而包含实质信息的数据则使用 tanh 函数作为激活函数。

- 遗忘门

  - ![截屏2021-01-13 上午11.48.02](flink%E8%BF%9B%E9%98%B6.assets/%E6%88%AA%E5%B1%8F2021-01-13%20%E4%B8%8A%E5%8D%8811.48.02.png)
  - $\boldsymbol{f}=\sigma\left(\boldsymbol{x}_{t} \boldsymbol{W}_{x}^{(\mathrm{f})}+\boldsymbol{h}_{t-1} \boldsymbol{W}_{h}^{(\mathrm{f})}+\boldsymbol{b}^{(\mathrm{f})}\right)$
  - $c_{t}=\boldsymbol{f} \odot \boldsymbol{c}_{t-1}$

- 新的记忆单元

  - ![截屏2021-01-13 上午11.51.03](flink%E8%BF%9B%E9%98%B6.assets/%E6%88%AA%E5%B1%8F2021-01-13%20%E4%B8%8A%E5%8D%8811.51.03-1286807.png)
  - 基于 tanh 节点计算出的结果被加到上一时刻的记忆单元 ct−1 上。这样一来，新的信息就被添加到了记忆单元中。这个 tanh 节点的作用不是门，而是将新的信息添加到记忆单元中。因此，它不用 sigmoid 函数作为激活函数，而是使用 tanh 函数。
  - $\boldsymbol{g}=\tanh \left(\boldsymbol{x}_{t} \boldsymbol{W}_{x}^{(\mathrm{g})}+\boldsymbol{h}_{t-1} \boldsymbol{W}_{h}^{(\mathrm{g})}+\boldsymbol{b}^{(\mathrm{g})}\right)$

- 输入门

  - ![截屏2021-01-13 上午11.52.48](flink%E8%BF%9B%E9%98%B6.assets/%E6%88%AA%E5%B1%8F2021-01-13%20%E4%B8%8A%E5%8D%8811.52.48.png)
  - 输入门判断新增信息 g 的各个元素的价值有多大。输入门不会不经考虑就添加新信息，而是会对要添加的信息进行取舍。换句话说，输入门会添加加权后的新信息。
  - $\boldsymbol{i}=\sigma\left(\boldsymbol{x}_{t} \boldsymbol{W}_{x}^{(\mathrm{i})}+\boldsymbol{h}_{t-1} \boldsymbol{W}_{h}^{(\mathrm{i})}+\boldsymbol{b}^{(\mathrm{i})}\right)$

  	- 然后，将 i 和 g 的对应元素的乘积添加到记忆单元中。

- LSTM 的梯度的流动

	- 我们仅关注记忆单元，绘制了它的反向传播。此时，记忆单元的反向传播仅流过“+”和“×”节点。“+”节点将上游传来的梯度原样流出，所以梯度没有变化(退化)。
- 而“×”节点的计算并不是矩阵乘积，而是对应元素的乘积(阿达玛积)。顺便说一下，在之前的 RNN 的反向传播中，我们使用相同的权重矩阵重复了多次矩阵乘积计算，由此导致了梯度消失(或梯度爆炸)。而这里的 LSTM 的反向传播进行的不是矩阵乘积计算，而是对应元素的乘积计算， 而且每次都会基于不同的门值进行对应元素的乘积计算。这就是它不会发生梯度消失(或梯度爆炸)的原因。
- “×”节点的计算由遗忘门控制(每次输出不同的门值)。遗忘门认为“应该忘记”的记忆单元的元素，其梯度会变小；而遗忘门认为“不能忘记”的元素，其梯度在向过去的方向流动时不会退化。因此，可以期待记忆单元的梯度(应该长期记住的信息)能在不发生梯度消失的情况下传播。
	
- LSTM 的实现

  - ```python
    class LSTM:
      def __init__(self, Wx, Wh, b):
        self.params = [Wx, Wh, b]
        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]
        self.cache = None
    
      def forward(self, x, h_prev, c_prev):
        Wx, Wh, b = self.params
        N, H = h_prev.shape
        A = np.dot(x, Wx) + np.dot(h_prev, Wh) + b
    		# slice
        f = A[:, :H]
        g = A[:, H:2*H]
        i = A[:, 2*H:3*H]
        o = A[:, 3*H:]
        
        f = sigmoid(f)
        g = np.tanh(g)
        i = sigmoid(i)
        o = sigmoid(o)
        
        c_next = f * c_prev + g * i
        h_next = o * np.tanh(c_next)
      	
        self.cache = (x, h_prev, c_prev, i, f, g, o, c_next)
        
        return h_next, c_next
    ```

  - 在 slice 节点的反向传播中，拼接 4 个矩阵。图中有 4 个梯度 df、dg、di 和 do，将它们拼接成 dA。

      - 截屏2021-01-13 下午2.38.18.png

      - ![截屏2021-01-13 下午2.38.18](flink%E8%BF%9B%E9%98%B6.assets/%E6%88%AA%E5%B1%8F2021-01-13%20%E4%B8%8B%E5%8D%882.38.18.png)

        - ```python
            dA = np.hstack((df, dg, di, do))
            ```

- Time LSTM 层的实现

  - ```python
    class TimeLSTM:
      def __init__(self, Wx, Wh, b, stateful=False):
        self.params = [Wx, Wh, b]
        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]
        self.layers = None
        self.h, self.c = None, None
        self.dh = None
        self.stateful = stateful
      
      def forward(self, xs):
        Wx, Wh, b = self.params
        N, T, D = xs.shape
        H = Wh.shape[0]
        self.layers = []
        hs = np.empty((N, T, H), dtype='f')
        
        if not self.stateful or self.h is None:
          self.h = np.zeros((N, H), dtype='f')
        if not self.stateful or self.c is None:
          self.c = np.zeros((N, H), dtype='f')
    
        for t in range(T):
        	layer = LSTM(*self.params)
        	self.h, self.c = layer.forward(xs[:, t, :], self.h, self.c)
          hs[:, t, :] = self.h
        	self.layers.append(layer)
        
      	return hs
    
      def backward(self, dhs):
        Wx, Wh, b = self.params
        N, T, H = dhs.shape
        D = Wx.shape[0]
        dxs = np.empty((N, T, D), dtype='f')
        dh, dc = 0, 0
        grads = [0, 0, 0]
        
        for t in reversed(range(T)):
        	layer = self.layers[t]
        	dx, dh, dc = layer.backward(dhs[:, t, :] + dh, dc)
          dxs[:, t, :] = dx
          
        for i, grad in enumerate(layer.grads):
        	grads[i] += grad
          
        for i, grad in enumerate(grads):
          self.grads[i][...] = grad
          
        self.dh = dh
          
        return dxs
    
      def set_state(self, h, c=None):
        self.h, self.c = h, c
        
      def reset_state(self):
      	self.h, self.c = None, None
    ```

- 使用 LSTM 的语言模型

  - ```python
      class Rnnlm:
        def __init__(self, vocab_size=10000, wordvec_size=100, hidden_size=100):
        	V, D, H = vocab_size, wordvec_size, hidden_size
          rn = np.random.randn
      		# 初始化权重
          embed_W = (rn(V, D) / 100).astype('f')
          lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')
          lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')
          lstm_b = np.zeros(4 * H).astype('f')
          affine_W = (rn(H, V) / np.sqrt(H)).astype('f')
          affine_b = np.zeros(V).astype('f')
      
      		# 生成层
      		self.layers = [
      			TimeEmbedding(embed_W),
      			TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True),
            TimeAffine(affine_W, affine_b)
      		]
      		self.loss_layer = TimeSoftmaxWithLoss()
          self.lstm_layer = self.layers[1]
      
      		# 将所有的权重和梯度整理到列表中
          self.params, self.grads = [], []
          for layer in self.layers:
            self.params += layer.params
            self.grads += layer.grads
            
      	def predict(self, xs):
      		for layer in self.layers:
      			xs = layer.forward(xs)
            
            return xs
          
      	def forward(self, xs, ts):
      		score = self.predict(xs)
      		loss = self.loss_layer.forward(score, ts)
          
          return loss
      
      	def backward(self, dout=1):
      		dout = self.loss_layer.backward(dout)
          
          for layer in reversed(self.layers):
      			dout = layer.backward(dout)
            
          return dout
        
      	def reset_state(self):
          self.lstm_layer.reset_state()
          
      	def save_params(self, file_name='Rnnlm.pkl'):
          with open(file_name, 'wb') as f:
      			pickle.dump(self.params, f)
            
      	def load_params(self, file_name='Rnnlm.pkl'):
          with open(file_name, 'rb') as f:
      			self.params = pickle.load(f)
      ```

- 进一步改进 RNNLM

  - dropout

  	- 如果在时序方向上插入 Dropout，那么当模型学习时，随着时间的推移，信息会渐渐丢失。也就是说，因 Dropout 产生的噪声会随时间成比例地积累。考虑到噪声的积累，最好不要在时间轴方向上插入 Dropout。因此，我们在深度方向(垂直方向)上插入 Dropout 层。
  	- 除了深度方向，变分 Dropout 也能用在时间方向上，从而进一步提高语言模型的精度。它的机制是同一层的 Dropout 使用相同的 mask。这里所说的 mask 是指决定是否传递数据的随机布尔值。

  		- 通过同一层的 Dropout 共用 mask，mask 被“固定”。 如此一来，信息的损失方式也被“固定”，所以可以避免常规 Dropout 发生的指数级信息损失。

  - 权重共享

  	- 绑定(共享)Embedding 层和 Affine 层的权重的技巧在于权重共享。通过在这两个层之间共享权重，可以大大减少学习的参数数量。尽管如此，它仍能提高精度。真可谓一石二鸟!

  		- 假设词汇量为 V， LSTM 的隐藏状态的维数为 H，则 Embedding 层的权重形状为 V × H， Affine 层的权重形状为 H × V。此时，如果要使用权重共享，只需将 Embedding 层权重的转置设置为 Affine 层的权重。这个非常简单的技巧可以带来出色的结果。

  - ```python
    class BetterRnnlm(BaseModel):
    	def __init__(self, vocab_size=10000, wordvec_size=650, hidden_size=650, dropout_ratio=0.5):
    		V, D, H = vocab_size, wordvec_size, hidden_size
        rn = np.random.randn
    		embed_W = (rn(V, D) / 100).astype('f')
    		lstm_Wx1 = (rn(D, 4 * H) / np.sqrt(D)).astype('f')
        lstm_Wh1 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')
        lstm_b1 = np.zeros(4 * H).astype('f')
    		lstm_Wx2 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')
        lstm_Wh2 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')
        lstm_b2 = np.zeros(4 * H).astype('f')
    		affine_b = np.zeros(V).astype('f')
    
    		# 3点改进!
    		self.layers = [
          TimeEmbedding(embed_W),
    			TimeDropout(dropout_ratio),
          TimeLSTM(lstm_Wx1, lstm_Wh1, lstm_b1, stateful=True),
          TimeDropout(dropout_ratio),
          TimeLSTM(lstm_Wx2, lstm_Wh2, lstm_b2, stateful=True),
          TimeDropout(dropout_ratio),
    			TimeAffine(embed_W.T, affine_b) # 权重共享!!
    		self.loss_layer = TimeSoftmaxWithLoss()
        self.lstm_layers = [self.layers[2], self.layers[4]]
        self.drop_layers = [self.layers[1], self.layers[3], self.layers[5]]
          
    		self.params, self.grads = [], []
        for layer in self.layers:
    				self.params += layer.params
          	self.grads += layer.grads
          
    	def predict(self, xs, train_flg=False):
          for layer in self.drop_layers:
    				layer.train_flg = train_flg
          for layer in self.layers:
    				xs = layer.forward(xs)
          
          return xs
          
    	def forward(self, xs, ts, train_flg=True):
          score = self.predict(xs, train_flg)
    			loss = self.loss_layer.forward(score, ts)
          
          return loss
          
      def backward(self, dout=1):
      	dout = self.loss_layer.backward(dout)
        for layer in reversed(self.layers):
      		dout = layer.backward(dout)
          
        return dout
          
      def reset_state(self):
      	for layer in self.lstm_layers:
      		layer.reset_state(
    ```

  - 叠加两个 Time LSTM 层， 使用 Time Dropout 层， 并在 Time Embedidng 层 和 Time Affine 层之间共享权重。

## 基于 RNN 生成文本

### 使用 RNN 生成文本的步骤 

- 语言模型输出了当给定单词 i 时下一个出现的单词的概率分布。那么，它如何生成下一个新单词呢?

	- 一种可能的方法是选择概率最高的单词。在这种情况下，因为选择的是概率最高的单词，所以结果能唯一确定。也就是说，这是一种“确定性的” 方法。另一种方法是“概率性地”进行选择。根据概率分布进行选择，这样概率高的单词容易被选到，概率低的单词难以被选到。在这种情况下，被选到的单词(被采样到的单词)每次都不一样。

		- 这里我们想让每次生成的文本有所不同，这样一来，生成的文本富有变化，会更有趣。因此，我们通过后一种方法(概率性地选择的方法)来选择单词。

### 文本生成的实现 

- ```python
class RnnlmGen(Rnnlm):
	def generate(self, start_id, skip_ids=None, sample_size=100):
		word_ids = [start_id]
		x = start_id
    
		while len(word_ids) < sample_size:
			x = np.array(x).reshape(1, 1)
        score = self.predict(x)
			p = softmax(score.flatten())
			sampled = np.random.choice(len(p), size=1, p=p)
	      
			if (skip_ids is None) or (sampled not in skip_ids):
	      x = sampled
				word_ids.append(int(x))
	      
	    return word_ids
	```
	
	- model 的 predict() 方法进行的是 mini-batch 处理，所以输入 x 必 须是二维数组。因此，即使在只输入 1 个单词 ID 的情况下，也要将它的批大小视为 1，并将其整理成形状为 1 × 1 的 NumPy 数组。

### seq2seq 模型

- seq2seq 的原理

	- seq2seq 模型也称为 Encoder-Decoder 模型。顾名思义，这个模型有两个模块 —— Encoder(编码器)和 Decoder(解码器)。编码器对输入数据进行编码，解码器对被编码的数据进行解码。
	- ![截屏2021-01-19 上午11.57.45](flink%E8%BF%9B%E9%98%B6.assets/%E6%88%AA%E5%B1%8F2021-01-19%20%E4%B8%8A%E5%8D%8811.57.45.png)
- 上图中使用了 <eos> 这一分隔符(特殊符号)。这个分隔符被用作通知解码器开始生成文本的信号。另外，解码器采样到<eos>出现为止，所以它也是结束信号。也就是说，分隔符<eos>可以用来指示解码器的“开始 / 结束”。在其他文献中，也有使用 <go>、 <start> 或者“_”(下划线)作为分隔符的例子。


- 可变长度的时序数据

	- 在使用批数据进行学习时，会一起处理多个样本。此时，(在我们的实现中)需要保证一个批次内各个样本的数据形状是一致的。

		- 在基于 mini-batch 学习可变长度的时序数据时，最简单的方法是使用填充(padding)。

			- 在多余位置插入无效字符(这里是空白字符)，从而使所有输入数据的长度对齐。
			- 像这样，通过填充对齐数据的大小，可以处理可变长度的时序数据。但是，因为使用了填充，seq2seq 需要处理原本不存在的填充用字符，所以如果追求严谨，使用填充时需要向 seq2seq 添加一些填充专用的处理。比如， 在解码器中输入填充时，不应计算其损失(这可以通过向 Softmax with Loss 层添加 mask 功能来解决)。再比如，在编码器中输入填充时，LSTM 层应按原样输出上一时刻的输入。这样一来，LSTM 层就可以像不存在填充一样对输入数据进行编码。

- seq2seq 的实现

  - Encoder

    - ```python
      class Encoder:
      	def __init__(self, vocab_size, wordvec_size, hidden_size):
      		V, D, H = vocab_size, wordvec_size, hidden_size
          rn = np.random.randn
          
      		embed_W = (rn(V, D) / 100).astype('f')
      		lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')
          lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')
          lstm_b = np.zeros(4 * H).astype('f')
          
      		self.embed = TimeEmbedding(embed_W)
      		self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=False)
          
      		self.params = self.embed.params + self.lstm.params
          self.grads = self.embed.grads + self.lstm.grads
          self.hs = None
          
          def forward(self, xs):
          	xs = self.embed.forward(xs)
            hs = self.lstm.forward(xs)
            self.hs = hs
            
          	return hs[:, -1, :]
        
      		def backward(self, dh):
          	dhs = np.zeros_like(self.hs)
            dhs[:, -1, :] = dh
          	dout = self.lstm.backward(dhs)
            dout = self.embed.backward(dout)
            
            return dout
      ```

      - 这次是有多个短时序数据的问题。因此，针对每个问题重设 LSTM 的隐藏状态(为 0 向量)。

  - Decoder

    - 在使用 RNN 进行文本生成时，学习时和生成时的数据输入方法不同。
    - 在学习时，因为已经知道正确解，所以可以整体地输入时序方向上的数据。
    	- 相对地，在推理时(生成新字符串时)，则只能输入第1个通知开始的分隔符(本次为“_”)。然后，基于输出采样 1 个字符，并将这个采样出来的字符作为下一个输入，如此重复该过程。
    - 如上所述，在解码器中，在学习时和在生成时处理 Softmax 层的方式 是不一样的。因此，Softmax with Loss 层交给此后实现的 Seq2seq 类处理。 如图所示，Decoder 类仅承担 Time Softmax with Loss 层之前的部分。

      - 截屏2021-01-19 下午12.34.58.png
      - ![截屏2021-01-19 下午12.34.58](flink%E8%BF%9B%E9%98%B6.assets/%E6%88%AA%E5%B1%8F2021-01-19%20%E4%B8%8B%E5%8D%8812.34.58.png)

    - 顺便说一句，在进行文本生成时，我们基于 Softmax 函数的概率分布进行了采样，因此生成的文本会随机变动。因为这次的问题是加法，所以我们想消除这种概率性的“波动”，生成“确定性的”答案。为此， 这次我们仅选择得分最高的字符。也就是说，是“确定性”地选择，而不是“概率性”地选择。下图显示了解码器生成字符串的过程。

      - 截屏2021-01-19 下午12.33.30.png
      - ![截屏2021-01-19 下午12.33.30](flink%E8%BF%9B%E9%98%B6.assets/%E6%88%AA%E5%B1%8F2021-01-19%20%E4%B8%8B%E5%8D%8812.33.30.png)
      - 如图所示，这里出现了新的 argmax 节点，这是获取最大值的索引(本例中是字符 ID)的节点。结构和上一节展示的文本生成时的结构相同。不过这次没有使用 Softmax 层，而是从 Affine 层输出的得分 中选择了最大值的字符 ID。

    - ```python
      class Decoder:
      	def __init__(self, vocab_size, wordvec_size, hidden_size):
      		V, D, H = vocab_size, wordvec_size, hidden_size
          rn = np.random.randn
      		embed_W = (rn(V, D) / 100).astype('f')
      		lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')
          lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')
          lstm_b = np.zeros(4 * H).astype('f')
      		affine_W = (rn(H, V) / np.sqrt(H)).astype('f')
          affine_b = np.zeros(V).astype('f')
          
          self.embed = TimeEmbedding(embed_W)
      		self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)
      		self.affine = TimeAffine(affine_W, affine_b)
          
      		self.params, self.grads = [], []
          
      		for layer in (self.embed, self.lstm, self.affine):
      			self.params += layer.params self.grads += layer.grads
            
      		def forward(self, xs, h): 
            self.lstm.set_state(h)
      			out = self.embed.forward(xs)
      			out = self.lstm.forward(out)
            score = self.affine.forward(out)
            
            return score
          
      		def backward(self, dscore):
      			dout = self.affine.backward(dscore)
            dout = self.lstm.backward(dout)
            dout = self.embed.backward(dout)
            
      			dh = self.lstm.dh
      			
            return dh
          
      		def generate(self, h, start_id, sample_size):
            sampled = []
      			sample_id = start_id
            self.lstm.set_state(h)
      			for _ in range(sample_size):
      				x = np.array(sample_id).reshape((1, 1))
              out = self.embed.forward(x)
      				out = self.lstm.forward(out)
      				score = self.affine.forward(out)
      				sample_id = np.argmax(score.flatten())
              sampled.append(int(sample_id))
      			return sampled
      ```

- Seq2seq 类 

	- ```python
	    class Seq2seq(BaseModel):
	    	def __init__(self, vocab_size, wordvec_size, hidden_size):
	    		V, D, H = vocab_size, wordvec_size, hidden_size
	        self.encoder = Encoder(V, D, H)
	    		self.decoder = Decoder(V, D, H)
	    		self.softmax = TimeSoftmaxWithLoss()
	        
	        # TimeSoftmaxWithLoss层并没有任务的参数，所以也不需要记录梯度
	    		self.params = self.encoder.params + self.decoder.params
	        self.grads = self.encoder.grads + self.decoder.grads
	    
	    	def forward(self, xs, ts):
	    		decoder_xs, decoder_ts = ts[:, :-1], ts[:, 1:]
	    		h = self.encoder.forward(xs)
	    		score = self.decoder.forward(decoder_xs, h)
	        loss = self.softmax.forward(score, decoder_ts)
	        
	        return loss
	      
	    	def backward(self, dout=1):
	    		dout = self.softmax.backward(dout)
	        dh = self.decoder.backward(dout)
	        dout = self.encoder.backward(dh)
	        
	        return dout
	      
	    	def generate(self, xs, start_id, sample_size):
	    		h = self.encoder.forward(xs)
	    		sampled = self.decoder.generate(h, start_id, sample_size)
	        
	        return sampled
	    ```

- seq2seq 的改进

  - 反转输入数据(Reverse) 

  	- 虽然反转数据的效果因任务而异，但是通常都会有好的结果。
  	- 为什么反转数据后，学习进展变快，精度提高了呢？虽然理论上不是很清楚，但是直观上可以认为，反转数据后梯度的传播可以更平滑。比如，考虑将“吾輩 は 猫 で ある”A 翻译成“I am a cat”这一问题，单词“吾
  輩”和单词“I”之间有转换关系。此时，从“吾輩”到“I”的路程必须经过“は”“猫”“で”“ある”这 4 个单词的 LSTM 层。因此，在反向传播时，梯度从“I”抵达“吾輩”，也要受到这个距离的影响。
  - 那么，如果反转输入语句，也就是变为“ある で 猫 は 吾輩”，结果会怎样呢？此时，“吾輩”和“I”彼此相邻，梯度可以直接传递。如此，因为通过反转，输入语句的开始部分和对应的转换后的单词之间的距离变近
  （这样的情况变多），所以梯度的传播变得更容易，学习效率也更高。
  - 不过，在反转输入数据后，单词之间的“平均”距离并不会发生改变。

  - 偷窥（Peeky）

    - 如前所述，编码器将输入语句转换为固定长度的向量 h，这个 h集中了解码器所需的全部信息。也就是说，它是解码器唯一的信息源。但是，当前的 seq2seq 只有最开始时刻的 LSTM 层利用了 h。我们能更加充分地利用这个 h 吗？
    - 为了达成该目标，seq2seq 的第二个改进方案就应运而生了。具体来说，就是将这个集中了重要信息的编码器的输出 h 分配给解码器的其他层。

    - 截屏2021-01-20 下午8.09.36.png
    	- ![截屏2021-01-20 下午8.09.36](flink%E8%BF%9B%E9%98%B6.assets/%E6%88%AA%E5%B1%8F2021-01-20%20%E4%B8%8B%E5%8D%888.09.36.png)

  - ```python
   class PeekyDecoder:
    def __init__(self, vocab_size, wordvec_size, hidden_size):
      V, D, H = vocab_size, wordvec_size, hidden_size
      rn = np.random.randn

      embed_W = (rn(V, D) / 100).astype('f')
      lstm_Wx = (rn( H + D , 4 * H) / np.sqrt(H + D)).astype('f')
      lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')
      lstm_b = np.zeros(4 * H).astype('f')
      affine_W = (rn( H + H , V) / np.sqrt(H + H)).astype('f')
      affine_b = np.zeros(V).astype('f')

      self.embed = TimeEmbedding(embed_W)
      self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)
      self.affine = TimeAffine(affine_W, affine_b)
      self.params, self.grads = [], []
      
       for layer in (self.embed, self.lstm, self.affine):
         self.params += layer.params
         self.grads += layer.grads
         
       self.cache = None
   
     def forward(self, xs, h):
       N, T = xs.shape
       N, H = h.shape
       self.lstm.set_state(h)
       
       out = self.embed.forward(xs)
       
       hs = np.repeat(h, T, axis=0).reshape(N, T, H)
       out = np.concatenate((hs, out), axis=2)
       out = self.lstm.forward(out)
       
       out = np.concatenate((hs, out), axis=2)
       score = self.affine.forward(out)
       self.cache = H
       
       return score
   ```

- seq2seq的应用

	- 聊天机器人
	-  算法学习
	-  自动图像描

## Attention

###  seq2seq存在的问题

- seq2seq 中使用编码器对时序数据进行编码，然后将编码信息传递给解码器。此时，编码器的输出是固定长度的向量。实际上，这个“固定长度”存在很大问题。因为固定长度的向量意味着，无论输入语句的长度如何（无论多长），都会被转换为长度相同的向量。

### 编码器的改进

- 到目前为止，我们都只将 LSTM 层的最后的隐藏状态传递给解码器，但是编码器的输出的长度应该根据输入文本的长度相应地改变。这是编码器的一个可以改进的地方。

  - ![截屏2021-01-21 下午3.16.23](flink%E8%BF%9B%E9%98%B6.assets/%E6%88%AA%E5%B1%8F2021-01-21%20%E4%B8%8B%E5%8D%883.16.23.png)
  - 使用各个时刻（各个单词）的隐藏状态向量，可以获得和输入的单词数相同数量的向量。在图 8-2 的例子中，输入了 5 个单词，此时编码器输出 5 个向量。这样一来，编码器就摆脱了“一个固定长度的向量”的制约。

###  解码器的改进

- 我们在进行翻译时，大脑做了什么呢？比如，在将“吾輩は猫である”这句话翻译为英文时，肯定要用到诸如“吾輩 = I”“猫 = cat”这样的知识。也就是说，可以认为我们是专注于某个单词（或者单词集合），随时对这个单词进行转换的。那么，我们可以在 seq2seq 中重现同样的事情吗？确切地说，我们可以让 seq2seq 学习“输入和输出中哪些单词与哪些单词有关”这样的对应关系吗？

  - 从现在开始，我们的目标是找出与“翻译目标词”有对应关系的“翻译源词”的信息，然后利用这个信息进行翻译。也就是说，我们的目标是仅关注必要的信息，并根据该信息进行时序转换。这个机制称为 Attention。

    - ![截屏2021-01-21 下午5.10.59](flink%E8%BF%9B%E9%98%B6.assets/%E6%88%AA%E5%B1%8F2021-01-21%20%E4%B8%8B%E5%8D%885.10.59.png)

- 对各个单词计算表示重要度的权重

  - ![截屏2021-01-21 下午3.56.16](flink%E8%BF%9B%E9%98%B6.assets/%E6%88%AA%E5%B1%8F2021-01-21%20%E4%B8%8B%E5%8D%883.56.16.png)
  - 这里使用了表示各个单词重要度的权重（记为 a）。此时，a 像概率分布一样，各元素是 0.0 ～ 1.0 的标量，总和是 1。
  - ![截屏2021-01-21 下午3.56.50](flink%E8%BF%9B%E9%98%B6.assets/%E6%88%AA%E5%B1%8F2021-01-21%20%E4%B8%8B%E5%8D%883.56.50.png)
  - 然后，计算这个表示各个单词重要度的权重和单词向量 hs 的加权和，可以获得目标向量。这如果我们仔细观察，就可以发现“吾輩”对应的权重为 0.8。这意味着上下文向量 c 中含有很多“吾輩”向量的成分，可以说这个加权和计算基本代替了“选择”向量的操作。假设“吾輩”对应的权重是 1，其他单词对应的权重是 0，那么这就相当于“选择”了“吾輩”向量。
  	  - 上下文向量 c 中包含了当前时刻进行变换（翻译）所需的信息。更确切地说，模型要从数据中学习出这种能力。

- ```python
 class WeightSum:
	def __init__(self):
		self.params, self.grads = [], []
		self.cache = None

	def forward(self, hs, a):
		N, T, H = hs.shape
		ar = a.reshape(N, T, 1).repeat(H, axis=2)
    
		t = hs * ar
		c = np.sum(t, axis=1)
    
		self.cache = (hs, ar)
    
		return c
  
	def backward(self, dc):
		hs, ar = self.cache
		N, T, H = hs.shape
 		dt = dc.reshape(N, 1, H).repeat(T, axis=1)
     
     # sum的反向传播
 		dar = dt * hs
 		dhs = dt * ar
     # repeat的反向传播
 		da = np.sum(dar, axis=2) 
     
 		return dhs, da
 ```
 
- 下面我们来看一下各个单词的权重 a 的求解方法。

  - 我们的目标是用数值表示这个 h 在多大程度上和 hs 的各个单词向量“相似”。
    有几种方法可以做到这一点，这里我们使用最简单的向量内积。

    - ![截屏2021-01-21 下午4.51.30](flink%E8%BF%9B%E9%98%B6.assets/%E6%88%AA%E5%B1%8F2021-01-21%20%E4%B8%8B%E5%8D%884.51.30.png)

  - ```python
   class AttentionWeight:
  	def __init__(self):
  		self.params, self.grads = [], []
  		self.softmax = Softmax()
  		self.cache = None
      
  	def forward(self, hs, h):
  		N, T, H = hs.shape
  		hr = h.reshape(N, 1, H).repeat(T, axis=1)
      
  		t = hs * hr
  		s = np.sum(t, axis=2)
  		a = self.softmax.forward(s)
      
  		self.cache = (hs, hr)
      
  		return a
    
  	def backward(self, da):
      hs, hr = self.cache
      N, T, H = hs.shape
       
       ds = self.softmax.backward(da)
       dt = ds.reshape(N, T, 1).repeat(H, axis=2)
       dhs = dt * hr
       dhr = dt * hs
       dh = np.sum(dhr, axis=1)
       
       return dhs, dh
   ```

- 下图显示了用于获取上下文向量 c 的计算图的全貌。我们已经分为Weight Sum 层和 Attention Weight 层进行了实现。重申一下，这里进行的计算是：Attention Weight 层关注编码器输出的各个单词向量 hs，并计算各个单词的权重 a；然后，Weight Sum 层计算 a 和 hs 的加权和，并输出上下文向量 c。我们将进行这一系列计算的层称为 Attention 层。

  - ![截屏2021-01-21 下午4.52.53](flink%E8%BF%9B%E9%98%B6.assets/%E6%88%AA%E5%B1%8F2021-01-21%20%E4%B8%8B%E5%8D%884.52.53.png)

  - ```python
   class Attention:
  def __init__(self):
  self.params, self.grads = [], []
  self.attention_weight_layer = AttentionWeight()
  self.weight_sum_layer = WeightSum()
  self.attention_weight = None
  def forward(self, hs, h):
  a = self.attention_weight_layer.forward(hs, h)
  out = self.weight_sum_layer.forward(hs, a)
  self.attention_weight = a
  return out
  def backward(self, dout):
  dhs0, da = self.weight_sum_layer.backward(dout)
  dhs1, dh = self.attention_weight_layer.backward(da)
  dhs = dhs0 + dhs1
   return dhs, dh
   ```

- 最后，我们将在时序方向上扩展的多个 Attention 层整体实现为 Time Attention 层。

  - ![截屏2021-01-21 下午5.15.00](flink%E8%BF%9B%E9%98%B6.assets/%E6%88%AA%E5%B1%8F2021-01-21%20%E4%B8%8B%E5%8D%885.15.00.png)

  - ```python
   class TimeAttention:
    def __init__(self):
      self.params, self.grads = [], []
      self.layers = None
      self.attention_weights = None
    
    def forward(self, hs_enc, hs_dec):
      N, T, H = hs_dec.shape
      out = np.empty_like(hs_dec)
      self.layers = []
      self.attention_weights = []
      
      for t in range(T):
      	layer = Attention()
      	out[:, t, :] = layer.forward(hs_enc, hs_dec[:,t,:])
      	self.layers.append(layer)
      	self.attention_weights.append(layer.attention_weight)

      return out

    def backward(self, dout):
      N, T, H = dout.shape
      dhs_enc = 0
      dhs_dec = np.empty_like(dout)
        
       for t in range(T):
         layer = self.layers[t]
         dhs, dh = layer.backward(dout[:, t, :])
         dhs_enc += dhs
         dhs_dec[:,t,:] = dh
           
       return dhs_enc, dhs_dec
   ```

### 带Attention的seq2seq的实现

- 编码器的实现

  - ```python
   class AttentionEncoder(Encoder):
  	def forward(self, xs):
  		xs = self.embed.forward(xs)
  		hs = self.lstm.forward(xs)
      
  	return hs

  	def backward(self, dhs):
  		dout = self.lstm.backward(dhs)
  			dout = self.embed.backward(dout)
      
  	return dout
  ```
   
   - 这个类和上一章实现的 Encoder 类几乎一样，唯一的区别是，Encoder 类的 forward() 方法仅返回 LSTM 层的最后的隐藏状态向量，而AttentionEncoder类则返回所有的隐藏状态向量。因此，这里我们继承上一章的 Encoder 类进行实现。

- 解码器的实现

  - 解码器的层结构

    - ![截屏2021-01-21 下午5.28.10](flink%E8%BF%9B%E9%98%B6.assets/%E6%88%AA%E5%B1%8F2021-01-21%20%E4%B8%8B%E5%8D%885.28.10.png)

  - ```python
   class AttentionDecoder:
    def __init__(self, vocab_size, wordvec_size, hidden_size):
       V, D, H = vocab_size, wordvec_size, hidden_size
       rn = np.random.randn
      
       embed_W = (rn(V, D) / 100).astype('f')
       lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')
       lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')
       lstm_b = np.zeros(4 * H).astype('f')
       affine_W = (rn(2*H, V) / np.sqrt(2*H)).astype('f')
       affine_b = np.zeros(V).astype('f')
      
       self.embed = TimeEmbedding(embed_W)
       self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)
       self.attention = TimeAttention()
       self.affine = TimeAffine(affine_W, affine_b)
      
       layers = [self.embed, self.lstm, self.attention, self.affine]
       self.params, self.grads = [], []
       for layer in layers:
          self.params += layer.params
       		self.grads += layer.grads
    
  	def forward(self, xs, enc_hs):
      h = enc_hs[:,-1]
      self.lstm.set_state(h)
      out = self.embed.forward(xs)
      dec_hs = self.lstm.forward(out)
       c = self.attention.forward(enc_hs, dec_hs)
       out = np.concatenate((c, dec_hs), axis=2)
       score = self.affine.forward(out)
       
       return score
   
     def backward(self, dscore):
     # 参照源代码
     
     def generate(self, enc_hs, start_id, sample_size):
     # 参照源代码
   ```
- seq2seq的实现
    - AttentionSeq2seq 类的实现也和上一章实现的 seq2seq 几乎一样。区别仅在于，编码器使用AttentionEncoder类，解码器使用AttentionDecoder类。因此，只要继承上一章的 Seq2seq 类，并改一下初始化方法，就可以实现
      AttentionSeq2seq 类。

    - ```python
       class AttentionSeq2seq(Seq2seq):
       	def __init__(self, vocab_size, wordvec_size, hidden_size):
       		args = vocab_size, wordvec_size, hidden_size
       		self.encoder = AttentionEncoder(*args)
       		self.decoder = AttentionDecoder(*args)
       		self.softmax = TimeSoftmaxWithLoss()
       		self.params = self.encoder.params + self.decoder.params
       		self.grads = self.encoder.grads + self.decoder.grads
       ```

- Attention的评价

	- 就最终精度来看，Attention 和 Peeky 取得了差不多的结果。但是，随着时序数据变长、变复杂，除了学习速度之外，Attention 在精度上也会变得更有优势。

- 关于Attention的其他话题

  - 双向 LSTM

    - ![截屏2021-01-21 下午6.22.46](flink%E8%BF%9B%E9%98%B6.assets/%E6%88%AA%E5%B1%8F2021-01-21%20%E4%B8%8B%E5%8D%886.22.46.png)
    - 双向 LSTM 在之前的 LSTM 层上添加了一个反方向处理的 LSTM 层。然后，拼接各个时刻的两个 LSTM 层的隐藏状态，将其作为最后的隐藏状态向量（除了拼接之外，也可以“求和”或者“取平均”等）。
    - 双向 LSTM 的实现非常简单。一种实现方式是准备两个 LSTM 层（本章中是 Time LSTM 层），并调整输入各个层的单词的排列。具体而言，其中一个层的输入语句与之前相同，这相当于从左向右处理输入语句的常规的LSTM 层。而另一个 LSTM 层的输入语句则按照从右到左的顺序输入。如果原文是“A B C D”，就改为“D C B A”。通过输入改变了顺序的输入语句，另一个 LSTM 层从右向左处理输入语句。之后，只需要拼接这两个LSTM 层的输出，就可以创建双向 LSTM 层。

  - Attention层的使用方法

    - ![截屏2021-01-21 下午6.28.42](flink%E8%BF%9B%E9%98%B6.assets/%E6%88%AA%E5%B1%8F2021-01-21%20%E4%B8%8B%E5%8D%886.28.42.png)
    - Attention 层的输出（上下文向量）被连接到了下一时刻的 LSTM 层的输入处。通过这种结构，LSTM 层得以使用上下文向量的信息。相对地，我们实现的模型则是 Affine 层使用了上下文向量。

  - seq2seq的深层化和 skip connection

    - 使用了3层LSTM层的带Attention的seq2seq

      - ![截屏2021-01-21 下午6.31.59](flink%E8%BF%9B%E9%98%B6.assets/%E6%88%AA%E5%B1%8F2021-01-21%20%E4%B8%8B%E5%8D%886.31.59.png)
      - 在上图的模型中，编码器和解码器使用了 3 层 LSTM 层。编码器和解码器中通常使用层数相同的 LSTM 层。另外，Attention 层的使用方法有许多变体。这里将解码器 LSTM 层的隐藏状态输入 Attention层，然后将上下文向量（Attention 层的输出）传给解码器的多个层（LSTM层和 Affine 层）。

    - 残差连接

      - ![截屏2021-01-21 下午6.34.07](flink%E8%BF%9B%E9%98%B6.assets/%E6%88%AA%E5%B1%8F2021-01-21%20%E4%B8%8B%E5%8D%886.34.07.png)
      - 如上图所示，所谓残差连接，就是指“跨层连接”。此时，在残差连接的连接处，有两个输出被相加。请注意这个加法（确切地说，是对应元素的加法）非常重要。因为加法在反向传播时“按原样”传播梯度，所以残差连接中的梯度可以不受任何影响地传播到前一个层。这样一来，即便加深了层，梯度也能正常传播，而不会发生梯度消失（或者梯度爆炸），学习可以顺利进行。
      	
      - 在时间方向上，RNN 层的反向传播会出现梯度消失或梯度爆炸的问题。梯度消失可以通过 LSTM、GRU 等 Gated RNN 应对，梯度爆炸可以通过梯度裁剪应对。而对于深度方向上的梯度消失，这里介绍的残差连接很有效。

- Attention的应用

  - GNMT（Google Neural Machine Translation，谷歌神经机器翻译系统）
    - ![截屏2021-01-21 下午6.40.09](flink%E8%BF%9B%E9%98%B6.assets/%E6%88%AA%E5%B1%8F2021-01-21%20%E4%B8%8B%E5%8D%886.40.09.png)
    - GNMT 和本章实现的带 Attention 的 seq2seq 一样，由编码器、解码器和 Attention 构成。不过，与我们的简单模型不同，这里可以看到许多为了提高翻译精度而做的改进，比如 LSTM 层的多层化、双向 LSTM（仅编码器的第 1 层）和 skip connection 等。另外，为了提高学习速度，还进行了多个 GPU 上的分布式学习。
    	- 除了上述在架构上下的功夫之外，GNMT 还进行了低频词处理、用于加速推理的量化（quantization）等工作。利用这些技巧，GNMT 获得了非常好的结果。

  - Transformer

    - RNN 需要基于上一个时刻的计算结果逐步进行计算，因此（基本）不可能在时间方向上并行计算 RNN。在使用了 GPU 的并行计算环境下进行深度学习时，这一点会成为很大的瓶颈，于是我们就有了避开 RNN 的动机。

    - 在这样的背景下，现在关于去除 RNN 的研究（可以并行计算的RNN 的研究）很活跃，其中一个著名的模型是 Transformer模型。
    - Transformer 是在“Attention is all you need”这篇论文中提出来的方法。如论文标题所示，Transformer 不用 RNN，而用 Attention 进行处理。
    	- 除了 Transformer 之外，还有多个研究致力于去除 RNN，比如用卷积层（Convolution 层）代替 RNN 的研究。这里我们不去探讨该研究的细节，基本上就是用卷积层代替 RNN 来构成 seq2seq，并据此实现并行计算。

    - Transformer 是基于 Attention 构成的，其中使用了 Self-Attention 技巧，这一点很重要。Self-Attention 直译为“自己对自己的 Attention”，也就是说，这是以一个时序数据为对象的 Attention，旨在观察一个时序数据中每个元素与其他元素的关系。
      - ![截屏2021-01-21 下午6.44.58](flink%E8%BF%9B%E9%98%B6.assets/%E6%88%AA%E5%B1%8F2021-01-21%20%E4%B8%8B%E5%8D%886.44.58.png)

    - Transformer的层结构

      - ![截屏2021-01-21 下午6.46.24](flink%E8%BF%9B%E9%98%B6.assets/%E6%88%AA%E5%B1%8F2021-01-21%20%E4%B8%8B%E5%8D%886.46.24.png)
      - Transformer 中用 Attention 代替了 RNN。实际上，编码器和解码器两者都使用了 Self-Attention。
      	- 图中的 Feed Forward层表示前馈神经网络（在时间方向上独立的网络）。具体而言，使用具有一
      个隐藏层、激活函数为 ReLU 的全连接的神经网络。另外，图中的 Nx 表示灰色背景包围的元素被堆叠了 N 次。
      - 上图显示的是简化了的 Transformer。实际上，除了这个架构外，Skip Connection、Layer Normalization等技巧也会被用到。其他常见的技巧还有，（并行）使用多个 Attention、编码时序数据的位置信息（Positional Encoding，位置编码）等。

  - NTM

    - RNN 和 LSTM 能够使用内部状态来存储时序数据，但是它们的内部状态长度固定，能塞入其中的信息量有限。因此，可以考虑在RNN 的外部配置存储装置（内存），适当地记录必要信息。

    - 在带 Attention 的 seq2seq 中，编码器对输入语句进行编码。然后，解码器通过 Attention 使用被编码的信息。这里需要注意的仍是 Attention 的存在。基于 Attention，编码器和解码器实现了计算机中的“内存操作”。换句话说，这可以解释为，编码器将必要的信息写入内存，解码器从内存中读取必要的信息。可见计算机的内存操作可以通过神经网络复现。我们可以立刻想到一个方法：在 RNN 的外部配置一个存储信息的存储装置，并使用 Attention向这个存储装置读写必要的信息。实际上，这样的研究有好几个，NTM（Neural Turing Machine，神经图灵机）就是其中比较有名的一个。

    - NTM的层结构：新出现了Write Head层和Read Head层，它们进行内存读写
      - ![截屏2021-01-21 下午6.57.54](flink%E8%BF%9B%E9%98%B6.assets/%E6%88%AA%E5%B1%8F2021-01-21%20%E4%B8%8B%E5%8D%886.57.54.png)
      - 上图是简化版的 NTM 的层结构。这里 LSTM 层是控制器，执行NTM 的主要处理。Write Head 层接收 LSTM 层各个时刻的隐藏状态，将必要的信息写入内存。Read Head 层从内存中读取重要信息，并传递给下一个时刻的 LSTM 层。
      	- 那么，图 8-41 的 Write Head 层和 Read Head 层如何进行内存操作呢？
          - 当然是使用 Attention。
      - 重申一下，在读取（或者写入）内存中某个地址上的数据时，我们需要先“选择”数据。这个选择操作自身是不能微分的，因此先使用 Attention 选择所有地址上的数据，再利用权重表示每个数
      据的贡献，这样就能够利用可微分的计算替代选择这个操作。
      - 为了模仿计算机的内存操作，NTM 的内存操作使用了两个 Attention，分别是“基于内容的 Attention”和“基于位置的 Attention”。基于内容的Attention 和我们之前介绍的 Attention 一样，用于从内存中找到某个向量（查询向量）的相似向量。而基于位置的 Attention 用于从上一个时刻关注的内存地址（内存的各个位置的权重）前后移动。这里我们省略对其技术细节的探讨，具体可以通过一维卷积运算实现。基于内存位置的移动功能，可以再现“一边前进（一个内存地址）一边读取”这种计算机特有的活动。

